{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>SubTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mitigating Ethical Risks in Generative AI: Str...</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Artificial Intelligence (AI) has been around f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DSC Weekly 2 January 2024</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Announcements Top Stories In-Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mastering IoT Data Management for Business Suc...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>In today’s tech-driven landscape, the prolifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Generative AI business model disruption: The N...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024 will be all about changing business model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GenAI: Synthesizing DNA Sequences with LLM Tec...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>When people talk about Large Language Models, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title        date  \\\n",
       "0           0  Mitigating Ethical Risks in Generative AI: Str...  2024-01-03   \n",
       "1           1                          DSC Weekly 2 January 2024  2024-01-02   \n",
       "2           2  Mastering IoT Data Management for Business Suc...  2024-01-01   \n",
       "3           3  Generative AI business model disruption: The N...  2024-01-01   \n",
       "4           4  GenAI: Synthesizing DNA Sequences with LLM Tec...  2024-01-01   \n",
       "\n",
       "                                            SubTitle  \n",
       "0  Artificial Intelligence (AI) has been around f...  \n",
       "1                 Announcements Top Stories In-Depth  \n",
       "2  In today’s tech-driven landscape, the prolifer...  \n",
       "3  2024 will be all about changing business model...  \n",
       "4  When people talk about Large Language Models, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datascience_df2.csv\")\n",
    "df2 = pd.read_csv(\"levelup_df3.csv\")\n",
    "df3= pd.read_csv(\"towards_datascience2.csv\")\n",
    "combine_df = pd.concat([df1,df2,df3])\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>SubTitle</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mitigating Ethical Risks in Generative AI: Str...</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Artificial Intelligence (AI) has been around f...</td>\n",
       "      <td>Mitigating Ethical Risks in Generative AI: Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DSC Weekly 2 January 2024</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Announcements Top Stories In-Depth</td>\n",
       "      <td>DSC Weekly 2 January 2024Announcements Top Sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mastering IoT Data Management for Business Suc...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>In today’s tech-driven landscape, the prolifer...</td>\n",
       "      <td>Mastering IoT Data Management for Business Suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Generative AI business model disruption: The N...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024 will be all about changing business model...</td>\n",
       "      <td>Generative AI business model disruption: The N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GenAI: Synthesizing DNA Sequences with LLM Tec...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>When people talk about Large Language Models, ...</td>\n",
       "      <td>GenAI: Synthesizing DNA Sequences with LLM Tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title        date  \\\n",
       "0           0  Mitigating Ethical Risks in Generative AI: Str...  2024-01-03   \n",
       "1           1                          DSC Weekly 2 January 2024  2024-01-02   \n",
       "2           2  Mastering IoT Data Management for Business Suc...  2024-01-01   \n",
       "3           3  Generative AI business model disruption: The N...  2024-01-01   \n",
       "4           4  GenAI: Synthesizing DNA Sequences with LLM Tec...  2024-01-01   \n",
       "\n",
       "                                            SubTitle  \\\n",
       "0  Artificial Intelligence (AI) has been around f...   \n",
       "1                 Announcements Top Stories In-Depth   \n",
       "2  In today’s tech-driven landscape, the prolifer...   \n",
       "3  2024 will be all about changing business model...   \n",
       "4  When people talk about Large Language Models, ...   \n",
       "\n",
       "                                                text  \n",
       "0  Mitigating Ethical Risks in Generative AI: Str...  \n",
       "1  DSC Weekly 2 January 2024Announcements Top Sto...  \n",
       "2  Mastering IoT Data Management for Business Suc...  \n",
       "3  Generative AI business model disruption: The N...  \n",
       "4  GenAI: Synthesizing DNA Sequences with LLM Tec...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['text'] = combine_df['title'] + combine_df['SubTitle']\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df.drop(columns=[\"Unnamed: 0\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df2 = combine_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>SubTitle</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitigating Ethical Risks in Generative AI: Str...</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Artificial Intelligence (AI) has been around f...</td>\n",
       "      <td>Mitigating Ethical Risks in Generative AI: Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DSC Weekly 2 January 2024</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Announcements Top Stories In-Depth</td>\n",
       "      <td>DSC Weekly 2 January 2024Announcements Top Sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mastering IoT Data Management for Business Suc...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>In today’s tech-driven landscape, the prolifer...</td>\n",
       "      <td>Mastering IoT Data Management for Business Suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generative AI business model disruption: The N...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024 will be all about changing business model...</td>\n",
       "      <td>Generative AI business model disruption: The N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GenAI: Synthesizing DNA Sequences with LLM Tec...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>When people talk about Large Language Models, ...</td>\n",
       "      <td>GenAI: Synthesizing DNA Sequences with LLM Tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date  \\\n",
       "0  Mitigating Ethical Risks in Generative AI: Str...  2024-01-03   \n",
       "1                          DSC Weekly 2 January 2024  2024-01-02   \n",
       "2  Mastering IoT Data Management for Business Suc...  2024-01-01   \n",
       "3  Generative AI business model disruption: The N...  2024-01-01   \n",
       "4  GenAI: Synthesizing DNA Sequences with LLM Tec...  2024-01-01   \n",
       "\n",
       "                                            SubTitle  \\\n",
       "0  Artificial Intelligence (AI) has been around f...   \n",
       "1                 Announcements Top Stories In-Depth   \n",
       "2  In today’s tech-driven landscape, the prolifer...   \n",
       "3  2024 will be all about changing business model...   \n",
       "4  When people talk about Large Language Models, ...   \n",
       "\n",
       "                                                text  \n",
       "0  Mitigating Ethical Risks in Generative AI: Str...  \n",
       "1  DSC Weekly 2 January 2024Announcements Top Sto...  \n",
       "2  Mastering IoT Data Management for Business Suc...  \n",
       "3  Generative AI business model disruption: The N...  \n",
       "4  GenAI: Synthesizing DNA Sequences with LLM Tec...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chesta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clean_text(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub('[^a-z]',\" \",x)\n",
    "    x = re.sub(' +',' ',x).strip()\n",
    "    words = x.split()\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words =[lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "        \n",
    "        \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df2['text'] = combine_df2['text'].apply(lambda x: simple_clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>SubTitle</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitigating Ethical Risks in Generative AI: Str...</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Artificial Intelligence (AI) has been around f...</td>\n",
       "      <td>mitigating ethical risk generative ai strategy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DSC Weekly 2 January 2024</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Announcements Top Stories In-Depth</td>\n",
       "      <td>dsc weekly january announcement top story depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mastering IoT Data Management for Business Suc...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>In today’s tech-driven landscape, the prolifer...</td>\n",
       "      <td>mastering iot data management business success...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generative AI business model disruption: The N...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024 will be all about changing business model...</td>\n",
       "      <td>generative ai business model disruption nyt la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GenAI: Synthesizing DNA Sequences with LLM Tec...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>When people talk about Large Language Models, ...</td>\n",
       "      <td>genai synthesizing dna sequence llm techniques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date  \\\n",
       "0  Mitigating Ethical Risks in Generative AI: Str...  2024-01-03   \n",
       "1                          DSC Weekly 2 January 2024  2024-01-02   \n",
       "2  Mastering IoT Data Management for Business Suc...  2024-01-01   \n",
       "3  Generative AI business model disruption: The N...  2024-01-01   \n",
       "4  GenAI: Synthesizing DNA Sequences with LLM Tec...  2024-01-01   \n",
       "\n",
       "                                            SubTitle  \\\n",
       "0  Artificial Intelligence (AI) has been around f...   \n",
       "1                 Announcements Top Stories In-Depth   \n",
       "2  In today’s tech-driven landscape, the prolifer...   \n",
       "3  2024 will be all about changing business model...   \n",
       "4  When people talk about Large Language Models, ...   \n",
       "\n",
       "                                                text  \n",
       "0  mitigating ethical risk generative ai strategy...  \n",
       "1    dsc weekly january announcement top story depth  \n",
       "2  mastering iot data management business success...  \n",
       "3  generative ai business model disruption nyt la...  \n",
       "4  genai synthesizing dna sequence llm techniques...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIDF : Term Frequency Inverse Document Frequency\n",
    "\n",
    "This is quite confusing but let's try to break and undertsand how it is relevant in our context of text generation in am ore heuristic approach.\n",
    "\n",
    "This helps in identifying the keywords by by taking into account not only how many times a given word **t** occurs in a given **document** but how frequently it does occur in other documents as well.\n",
    "\n",
    "Majorly uses for text retrival as well as text mining that helps in automatic detection of the keywords.\n",
    "\n",
    "The algorithm does use .three numerical characteristics:-\n",
    "\n",
    "* number of time tf(t,d) that word t occurs in a document d, this number is known as **term frequency**\n",
    "\n",
    "* the total number **N** of documents in a given corpus D\n",
    "\n",
    "* number of documents df(t) which contains the given term t; this number is known as **document frequency**.\n",
    "\n",
    "Based on last two characteristic swe do get the **idf** N/df(t)\n",
    "\n",
    "Keywords characterizing the given document d we select words t with the largest value of product tf-idf(t,d,D) = tf(t,d). idf(t,D)\n",
    "\n",
    "This leads to adequate selection of the keywords.\n",
    "\n",
    "Here the model will assign each occurrence of word t in the corpus randomly assigned to one of the N documents in the corpus. The probability of single occurrence of t to a specific document will be 1/N; why because there are N documents and we are assigning equal probability.\n",
    "\n",
    "Probability estimation is under random assignment model a specific document d ends up with exactly k occurrences of word t.\n",
    "\n",
    "n = tf(t,D) total number of times t appear in the entire corpus.\n",
    "k = tf(t,d) number of times t appear in the document.\n",
    "\n",
    "This is in realtionship with the **Tf-IDF** and Keyword Importance\n",
    "Significance of word is determined via not just frequency of word in that document but also how unique that word is across the entire corpus.\n",
    "\n",
    "Words that are common across many documents are deemed less but words which are frequent in few documents are copnsidered more important.\n",
    "\n",
    "Why we are using this probablistic approach is to think Tf-IDF in a manner where we are thinking about randomness of the word distribution and to assess the importance of words in the documents based on the likelihood of their distribution pattern.\n",
    "\n",
    "Linking the paper that help you to understand the concept of TF-IDF in a detailed way \n",
    "\n",
    "https://www.tandfonline.com/doi/full/10.1080/03081079.2017.1291635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 2559)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_mat = vectorizer.fit_transform(combine_df2['text'].values)\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"artificial intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF has been chosen to find the keywords for each document present in the corpus.\n",
    "\n",
    "Now to generate the text of the given query; we'll be taking the advantage of Cosine Similarity that helps in determine how similar two documenst are to each other.\n",
    "\n",
    "Basic Concepts\n",
    "\n",
    "**Vector Space Model** : - text is going to be represented in a vector space. Value in each dimension represents the importance or frquency of that word in the text.\n",
    "\n",
    "**Word Vectors** :- For individual words, vector representation like Glove, Word2Vec are often used. these vectors capture semantic meanings of words based on their context in a large corpus.\n",
    "\n",
    "#### Cosine Similarity\n",
    "\n",
    "* Measures the cosine of angle between two non-zero vectors in a multi-dimensional space. We calculate it in terms of **dot** product of vectors divided by product of their magnitude.\n",
    "\n",
    "* It helps us to quantify how similar they are in terms of their semantic meanings.\n",
    "\n",
    "**Interpret the Results**\n",
    "\n",
    "* Cosine similarity of 1 means vectors are identical\n",
    "* Cosine similarity of -1 is they are completely dissimilar\n",
    "* Cosine similarity of 0 means there is no significant similarity between the two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles similar to the query: 0      mitigating ethical risk generative ai strategy...\n",
      "16     ai justice brave new world part ai governancei...\n",
      "182    gen ai framework tool every ai ml engineer kno...\n",
      "214    google gemini ai v chatgpt ai deep dive titan ...\n",
      "8      creating fair prosperous brave new world ai su...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Vectorize the titles using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combine_df2['text'])  # Assuming 'title' is the column name\n",
    "\n",
    "# Function to find similar titles based on a query\n",
    "def find_similar_titles(query, top_n=5):\n",
    "    query_tfidf = tfidf_vectorizer.transform([query])\n",
    "    cos_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "    similar_indices = cos_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    similar_titles = combine_df2['text'].iloc[similar_indices]\n",
    "    return similar_titles\n",
    "\n",
    "# Example Query\n",
    "query = \"ai models\"  # Replace with your query\n",
    "similar_titles = find_similar_titles(query)\n",
    "print(\"Titles similar to the query:\", similar_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clean_text(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub('[^a-z]',\" \",x)\n",
    "    x = re.sub(' +',' ',x).strip()\n",
    "    words = x.split()\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words =[lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "        \n",
    "        \n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df2['text'] = combine_df2['text'].apply(lambda x: simple_clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176785, 189840)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#create the model\n",
    "\n",
    "word2vec_model = Word2Vec(min_count=0,workers=8,vector_size=15)\n",
    "\n",
    "#prepare the vocab\n",
    "\n",
    "word2vec_model.build_vocab(combine_df2['text'].values)\n",
    "\n",
    "#train the model\n",
    "\n",
    "word2vec_model.train(combine_df2['text'].values,total_examples=word2vec_model.corpus_count,epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_sample = list(word2vec_model.wv.index_to_key[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.1178092 ,  0.06370014,  2.3720043 , -2.6711257 , -0.10399573,\n",
       "         0.7032753 ,  0.05867551,  1.3012085 ,  0.2515336 ,  0.9431123 ,\n",
       "         3.541885  ,  1.3180712 ,  0.6709672 , -1.1814612 , -3.017418  ],\n",
       "       dtype=float32),\n",
       " array([-0.9594652 ,  0.15246652,  1.963687  , -2.4227076 , -0.07080292,\n",
       "         0.7337299 ,  0.00429893,  1.0033599 ,  0.18997736,  0.9089392 ,\n",
       "         2.948596  ,  1.0328969 ,  0.73770064, -1.0407968 , -2.5568447 ],\n",
       "       dtype=float32),\n",
       " array([-0.99132276,  0.06777174,  1.9525964 , -2.3286557 , -0.02881214,\n",
       "         0.7018621 ,  0.10257982,  0.9631509 ,  0.19499597,  0.75778484,\n",
       "         2.747382  ,  1.1561697 ,  0.5004294 , -1.0920178 , -2.4527543 ],\n",
       "       dtype=float32),\n",
       " array([-6.8087125e-01,  7.5712259e-04,  1.7312801e+00, -1.8385310e+00,\n",
       "         1.0245566e-01,  4.0995201e-01,  4.6106651e-02,  8.2870197e-01,\n",
       "         2.4044864e-01,  8.7641740e-01,  2.4217796e+00,  9.9158388e-01,\n",
       "         4.5506853e-01, -9.3280286e-01, -1.9519191e+00], dtype=float32),\n",
       " array([-0.72905314,  0.06745455,  1.4901584 , -1.9031333 ,  0.00436337,\n",
       "         0.49528447,  0.02676523,  0.8677621 ,  0.1851187 ,  0.72500235,\n",
       "         2.3476381 ,  0.94263965,  0.47261083, -0.8883135 , -1.8413932 ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_sample = [word2vec_model.wv[word] for word in vocabulary_sample]\n",
    "vector_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['data', 'ai', 'guide', 'part', 'learning'],\n",
       " [array([-1.1178092 ,  0.06370014,  2.3720043 , -2.6711257 , -0.10399573,\n",
       "          0.7032753 ,  0.05867551,  1.3012085 ,  0.2515336 ,  0.9431123 ,\n",
       "          3.541885  ,  1.3180712 ,  0.6709672 , -1.1814612 , -3.017418  ],\n",
       "        dtype=float32)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_sample, vector_sample[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Vocabulary Size': 2573,\n",
       " 'Sample Words': ['data',\n",
       "  'ai',\n",
       "  'guide',\n",
       "  'part',\n",
       "  'learning',\n",
       "  'using',\n",
       "  'model',\n",
       "  'python',\n",
       "  'code',\n",
       "  'llm'],\n",
       " \"Vector for a Sample Word ('data')\": array([-1.1178092 ,  0.06370014,  2.3720043 , -2.6711257 , -0.10399573,\n",
       "         0.7032753 ,  0.05867551,  1.3012085 ,  0.2515336 ,  0.9431123 ,\n",
       "         3.541885  ,  1.3180712 ,  0.6709672 , -1.1814612 , -3.017418  ],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wv_contents = {\n",
    "    \"Vocabulary Size\": len(word2vec_model.wv),\n",
    "    \"Sample Words\": word2vec_model.wv.index_to_key[:10],  # First 10 words in the vocabulary\n",
    "    \"Vector for a Sample Word ('data')\": word2vec_model.wv['data'] if 'data' in word2vec_model.wv else \"Not in Vocabulary\"\n",
    "}\n",
    "\n",
    "model_wv_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sentence = \"machine learning trends\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine', 'learning', 'trends']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = query_sentence.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_vector(sentence,model):\n",
    "    words = sentence.split()\n",
    "    # print(words)\n",
    "\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "\n",
    "    if len(word_vectors)==0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(word_vectors,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = average_vector(query_sentence,word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectors = [average_vector(\" \".join(row), word2vec_model) for row in combine_df2.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021957703"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(title_vectors[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities =[cosine_similarity([query_vector], [title_vector])[0][0] for title_vector in title_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_with_similarities = list(zip(combine_df2['text'],similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_titles = sorted(titles_with_similarities,key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding machine learning paradigm comparative study supervised unsupervised algorithmsintroduction: 0.9997289776802063\n",
      "machine learning need case study signature detectionthe need insight: 0.9997285604476929\n",
      "boosting algorithm machine learning part adaboostyou throw data scientist: 0.9997137188911438\n",
      "title mastering unit test python part mock mocker quick great tool want leave: 0.9996119737625122\n",
      "learning discrete data harmonium part essentialsunderstanding logic behind adaboost: 0.9995943903923035\n"
     ]
    }
   ],
   "source": [
    "for title in range(len(sorted_titles[:5])):\n",
    "    print(f'{\" \".join(sorted_titles[title][0])}: {(sorted_titles[title][1])}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_clust_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
